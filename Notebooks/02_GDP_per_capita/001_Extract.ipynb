{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4c86ac1-86d9-4770-a175-ef502be66a1a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# v0: XLSX -> CSV (abas Carga1 e Carga2) sem libs externas\n",
    "import os, io, csv, re, zipfile, xml.etree.ElementTree as ET\n",
    "from urllib.request import urlopen\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "spark.conf.set(\"spark.sql.session.timeZone\", \"America/Sao_Paulo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b8ff0183-9bd6-4333-8427-e4a431f0dab2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Download do XLSX\n",
    "xlsx_url = \"https://raw.githubusercontent.com/JDLNas/curso_formacao_microsoft_powerbi_profissional/main/Fontes/EXCEL/000_COMPLETAR.xlsx\"\n",
    "processar_path = \"/Volumes/fomacao_microsoft_power_bi_profisional/bronze/landing/processar/000_COMPLETAR.xlsx\"\n",
    "\n",
    "with open(processar_path, \"wb\") as f:\n",
    "    f.write(requests.get(xlsx_url).content)\n",
    "\n",
    "# Use Spark to read XLSX and write CSV (no need for openpyxl or pandas)\n",
    "for sheet in ['Carga1', 'Carga2']:\n",
    "    df = spark.read.format(\"com.crealytics.spark.excel\") \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .option(\"dataAddress\", f\"'{sheet}'!A1\") \\\n",
    "        .load(processar_path)\n",
    "    csv_path = f\"/Volumes/fomacao_microsoft_power_bi_profisional/bronze/landing/processados/{sheet}.csv\"\n",
    "    df.write.mode(\"overwrite\").option(\"header\", \"true\").csv(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a43aa8b-d25d-471e-8cf9-cb1086028ee3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ---- Parâmetros ----\n",
    "xlsx_url = \"https://raw.githubusercontent.com/JDLNas/curso_formacao_microsoft_powerbi_profissional/main/Fontes/EXCEL/000_COMPLETAR.xlsx\"\n",
    "processar_path = \"/Volumes/fomacao_microsoft_power_bi_profisional/bronze/landing/processar/000_COMPLETAR.xlsx\"\n",
    "processados_dir = \"/Volumes/fomacao_microsoft_power_bi_profisional/bronze/landing/processados\"\n",
    "file_stem = os.path.splitext(os.path.basename(processar_path))[0]  # 000_COMPLETAR\n",
    "\n",
    "# ---- Baixar XLSX ----\n",
    "local_xlsx = f\"{processar_path}\"\n",
    "os.makedirs(os.path.dirname(local_xlsx), exist_ok=True)\n",
    "with open(local_xlsx, \"wb\") as f:\n",
    "    f.write(urlopen(xlsx_url).read())\n",
    "\n",
    "# ---- Preparar saída ----\n",
    "out_dir_local = f\"{processados_dir}\"\n",
    "os.makedirs(out_dir_local, exist_ok=True)\n",
    "\n",
    "# ---- Namespaces do Excel ----\n",
    "NS = {'m': 'http://schemas.openxmlformats.org/spreadsheetml/2006/main'}\n",
    "\n",
    "# ---- Abrir XLSX (é um .zip) ----\n",
    "with zipfile.ZipFile(local_xlsx) as z:\n",
    "    # sharedStrings (texto de células 's')\n",
    "    shared = []\n",
    "    if \"xl/sharedStrings.xml\" in z.namelist():\n",
    "        ss_xml = ET.fromstring(z.read(\"xl/sharedStrings.xml\"))\n",
    "        for si in ss_xml.findall(\"m:si\", NS):\n",
    "            chunks = []\n",
    "            t = si.find(\"m:t\", NS)\n",
    "            if t is not None:\n",
    "                chunks.append(t.text or \"\")\n",
    "            else:\n",
    "                for r in si.findall(\"m:r\", NS):\n",
    "                    rt = r.find(\"m:t\", NS)\n",
    "                    if rt is not None:\n",
    "                        chunks.append(rt.text or \"\")\n",
    "            shared.append(\"\".join(chunks))\n",
    "\n",
    "    # Mapeia nome de aba -> path do worksheet\n",
    "    wb_xml   = ET.fromstring(z.read(\"xl/workbook.xml\"))\n",
    "    rels_xml = ET.fromstring(z.read(\"xl/_rels/workbook.xml.rels\"))\n",
    "    rid_to_target = {rel.attrib['Id']: rel.attrib['Target']\n",
    "                     for rel in rels_xml.findall('.//{http://schemas.openxmlformats.org/package/2006/relationships}Relationship')}\n",
    "    sheet_to_path = {}\n",
    "    for sh in wb_xml.findall(\"m:sheets/m:sheet\", NS):\n",
    "        name = sh.attrib[\"name\"]\n",
    "        rid  = sh.attrib['{http://schemas.openxmlformats.org/officeDocument/2006/relationships}id']\n",
    "        target = rid_to_target[rid]\n",
    "        sheet_to_path[name] = f\"xl/{target}\" if not target.startswith(\"xl/\") else target\n",
    "\n",
    "    def col_letters_to_idx(letters: str) -> int:\n",
    "        n = 0\n",
    "        for ch in letters:\n",
    "            n = n*26 + (ord(ch)-64)  # 'A'->1\n",
    "        return n-1\n",
    "\n",
    "    def export_sheet(sheet_name: str, out_csv_path: str):\n",
    "        if sheet_name not in sheet_to_path:\n",
    "            raise FileNotFoundError(f\"Aba '{sheet_name}' não encontrada no XLSX.\")\n",
    "        ws = ET.fromstring(z.read(sheet_to_path[sheet_name]))\n",
    "\n",
    "        rows, max_col = [], 0\n",
    "        for row in ws.findall(\".//m:sheetData/m:row\", NS):\n",
    "            vals = {}\n",
    "            for c in row.findall(\"m:c\", NS):\n",
    "                ref = c.attrib.get(\"r\",\"A1\")  # p.ex. C5\n",
    "                m = re.match(r\"([A-Z]+)(\\d+)\", ref)\n",
    "                col_idx = col_letters_to_idx(m.group(1)) if m else 0\n",
    "                ctype = c.attrib.get(\"t\")\n",
    "                v = c.find(\"m:v\", NS)\n",
    "                is_node = c.find(\"m:is/m:t\", NS)\n",
    "\n",
    "                val = \"\"\n",
    "                if is_node is not None:\n",
    "                    val = is_node.text or \"\"\n",
    "                elif v is not None:\n",
    "                    raw = v.text or \"\"\n",
    "                    if ctype == \"s\":                 # sharedStrings\n",
    "                        i = int(raw) if raw.isdigit() else -1\n",
    "                        val = shared[i] if 0 <= i < len(shared) else \"\"\n",
    "                    elif ctype == \"b\":               # boolean\n",
    "                        val = \"TRUE\" if raw == \"1\" else \"FALSE\"\n",
    "                    else:                            # número/string/fórmula (valor em cache)\n",
    "                        val = raw\n",
    "\n",
    "                vals[col_idx] = val\n",
    "                if col_idx+1 > max_col: max_col = col_idx+1\n",
    "            rows.append([vals.get(i, \"\") for i in range(max_col)])\n",
    "\n",
    "        header = rows[0] if rows else []\n",
    "        body = rows[1:] if len(rows) > 1 else []\n",
    "\n",
    "        with open(out_csv_path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "            w = csv.writer(f)\n",
    "            if header: w.writerow(header)\n",
    "            for r in body:\n",
    "                if len(r) < max_col: r = r + [\"\"]*(max_col-len(r))\n",
    "                w.writerow(r)\n",
    "\n",
    "    # Exporta as duas abas\n",
    "    export_sheet(\"Carga1\", f\"{out_dir_local}/{file_stem}__Carga1.csv\")\n",
    "    export_sheet(\"Carga2\", f\"{out_dir_local}/{file_stem}__Carga2.csv\")\n",
    "\n",
    "print(\"CSVs salvos em:\")\n",
    "print(f\" - {processados_dir}/{file_stem}__Carga1.csv\")\n",
    "print(f\" - {processados_dir}/{file_stem}__Carga2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bad2e4a5-9360-431e-a9cb-2aeb544db3d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "base = f\"{processados_dir}\"\n",
    "df_carga1 = (spark.read\n",
    "                  .option(\"header\",\"true\").csv(f\"{base}/{file_stem}__Carga1.csv\")\n",
    "                  .withColumn('_ingest_ts_utc', F.current_timestamp())\n",
    "                  .withColumn(\"_ingest_date\", F.to_date(F.col(\"_ingest_ts_utc\")))\n",
    "                  .withColumn(\"_source_path\", F.col(\"_metadata.file_path\"))\n",
    "                  .withColumn(\"_source_file\", F.regexp_extract(F.col(\"_source_path\"), r\"([^/]+)$\", 1))\n",
    "                  .withColumnRenamed(\"Dados do PIB\", \"Dados_do_PIB\"))\n",
    "\n",
    "\n",
    "df_carga2 = (spark.read\n",
    "                  .option(\"header\",\"true\").csv(f\"{base}/{file_stem}__Carga2.csv\")\n",
    "                  .withColumn('_ingest_ts_utc', F.current_timestamp())\n",
    "                  .withColumn(\"_ingest_date\", F.to_date(F.col(\"_ingest_ts_utc\")))\n",
    "                  .withColumn(\"_source_path\", F.col(\"_metadata.file_path\"))\n",
    "                  .withColumn(\"_source_file\", F.regexp_extract(F.col(\"_source_path\"), r\"([^/]+)$\", 1)))\n",
    "\n",
    "\n",
    "# display(df_carga1); display(df_carga2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd49af4f-4137-4993-b1ba-5f3ba4f4477b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define nomes das tabelas Delta para cada carga\n",
    "tbl_carga1 = '__Carga1'\n",
    "tbl_carga2 = '__Carga2'\n",
    "\n",
    "# Define caminhos de destino para as tabelas Delta\n",
    "destino_path_carga1 = f'fomacao_microsoft_power_bi_profisional.bronze.{tbl_carga1}'\n",
    "destino_path_carga2 = f'fomacao_microsoft_power_bi_profisional.bronze.{tbl_carga2}'\n",
    "\n",
    "# Escreve df_carga1 como tabela Delta particionada por _ingest_date, sobrescrevendo se já existir\n",
    "df_bronze = (df_carga1.write\n",
    "             .format('delta')\n",
    "             .mode('overwrite')\n",
    "             .option('overwriteSchema', 'true')\n",
    "             .partitionBy('_ingest_date')\n",
    "             .saveAsTable(destino_path_carga1))\n",
    "\n",
    "# Escreve df_carga2 como tabela Delta particionada por _ingest_date, sobrescrevendo se já existir\n",
    "df_bronze = (df_carga2.write\n",
    "             .format('delta')\n",
    "             .mode('overwrite')\n",
    "             .option('overwriteSchema', 'true')\n",
    "             .partitionBy('_ingest_date')\n",
    "             .saveAsTable(destino_path_carga2))\n",
    "\n",
    "# Exibe a contagem de registros em cada tabela Delta criada\n",
    "print(f'Contagem de registros: {spark.table(destino_path_carga1).count()}')\n",
    "print(f'Contagem de registros: {spark.table(destino_path_carga2).count()}')"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "001_Extract",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
